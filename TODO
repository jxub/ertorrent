tODO:
=======

Questions:
- Should cache be handled centralized or distributed?
- If I have a supervision tree a supervisor, supervising second supervisor and
  a gen_server. How would I go about to give, the PID of the second supervisor,
  as an argument to the gen_server?
- Are the three steps a sufficient boot separation? Check ertorrent_app.erl

General:
- Figure out the piece format of the metainfo
- Merge the peer_in_sup with the peer_ssup
- Read up on OS-specifics http://erlang.org/doc/man/os.html#type-0
- Figure out start up order. settings sup.tree needs to start up first so that
  torrent_ssup can get port settings.
- Create peer_ssup
- Create file_ssup
- Investigate of there should be a sup for hash otherwise rename sup -> ssup
- Investigate of there should be a sup for tracker otherwise rename sup -> ssup
- Update ertorrent_sup with new ssups
- Move knowledge about the incoming peer port form the torrent sup.tree. It was
  originially placed there, so that the torrent_worker could embed it into the
  tracker messages. The torrent_worker should only send the tracker_worker with
  the torrent relevant parameters. The tracker tree and the peer tree should
  know about this port.
- Determine a file format for cached metainfo (e.g. JSON, kept in a seperate
  file?)
- Determine a term format for cached metainfo (e.g. {metainfo_cache,
  [MetainfoX, MetainfoY]} ?)
- Separate the tracker dispatcher from the http dispatcher, to create a generic
  http_dispatcher.
- In order to run on a distributed system, the ssup have to start the srv which
  in return calls start_child to the ssup to get the PID of sup.
- Look over how the peer sockets are being set up. It could be an idea to use
  the IP address as ID for the peer_workers but how to we retreive the address
  if someone else is establishing the connection?
- Figure out how to store the uploaded amount of each torrent since this is a
  part of the tracker announcement

file_srv:
- Implement handle_info({file_w_pread_resp

file_worker:
- handle_cast({pread add Fd to the cache
- In terminate, close all the cached Fds
- Unify variable naming, Offset/Begin

settings_srv:
- A setting for peer port

tracker_srv:
- Add incoming peer port

torrent_worker:
- Figure out the time between tracker announcements from the specification and
  update the value of ANNOUNCE_TIME
- During "handle_cast({start", request a tracker_worker
- During "handle_cast({start", create a file_worker
- Create missing files based on Missing_files
- Remove port from announce_trigger()
- Don't serve buffered requests when peer is choked.
- Decide how to handle buffered requests during the time that the peer is
  choked. Drop all the requests or keep them buffered?
- What is the compact field in the state record for?

torrent_srv:
- Iterate and add stored metainfo in init/1, stored metainfo should be read by
  settings_srv
- Add functionality to pause
- Add functionality to remove torrents NOTE: Remember to remove it from the
  cache and re-write the cache file.

peer_accept:
- Retreive the port from the settings_srv during init/1
- Handle Reason in terminate/2

peer_worker:
- Add serialization of Data in handle_info({peer_srv_piece, ...})
- handle_info({peer_srv_piece, ...}) fix return value
- Check difference between handle_info({peer_srv_piece, Data} and
  handle_info({peer_srv_piece, _Index, _Hash, Data}, State).
- Look up if it's necessary to verify the Socket in messages from gen_tcp, e.g.
  handle_info({tcp, Socket, Data})
- handle_info({tcp, _S, <<19:32, "BitTorrent protocol", 0:64, Info_hash:160,
  Peer_id:160>>}, State), add verification of the Peer_id
- Figure out when it is safe to clear out cached pieces (when the pieces are
  completely transfered to the peer). Is it safe to assume that we'll receive a
  HAVE message?
- Rename incoming/outgoing piece entries in the state record to rx/tx
- Rename cached_requests in the state record to buffered_requests
- Add default values to the peer_state record since this is covered by the
  bittorrent protocol specification

utils:
- Sort the functions in this module
